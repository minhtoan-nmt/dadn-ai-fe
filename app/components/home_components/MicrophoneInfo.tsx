import { useEffect, useState, useRef } from "react";
import { GrMicrophone } from "react-icons/gr";

export default function MicrophoneInfo() {
    const [isRecording, setIsRecording] = useState(false);
    const [transcript, setTranscript] = useState("");
    
    // 1. Th√™m state ƒë·ªÉ l∆∞u k·∫øt qu·∫£ tr·∫£ v·ªÅ t·ª´ API AI
    const [apiResult, setApiResult] = useState<any>(null); 

    const transcriptRef = useRef(""); 
    const recognitionRef = useRef<any>(null);

    useEffect(() => {
        const SpeechRecognition =
            (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;

        if (!SpeechRecognition) {
            alert("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ SpeechRecognition API");
            return;
        }

        const recognition = new SpeechRecognition();
        recognition.lang = "vi-VN";            
        recognition.continuous = true;          
        recognition.interimResults = false;     

        recognition.onresult = (event: any) => {
            let chunk = "";
            for (let i = event.resultIndex; i < event.results.length; i++) {
                if (event.results[i].isFinal) {
                    chunk += event.results[i][0].transcript + " ";
                }
            }

            if (chunk.trim() !== "") {
                console.log("ü¶ª V·ª´a nghe ƒë∆∞·ª£c:", chunk);
                setTranscript((prev) => {
                    const newText = prev + chunk;
                    transcriptRef.current = newText;
                    return newText;
                });
            }
        };

        recognition.onerror = (e: any) => console.error("Speech error:", e);
        recognitionRef.current = recognition;
    }, []);

    const handleToggleRecord = async () => {
        if (recognitionRef.current) {
            if (!isRecording) {
                // --- B·∫ÆT ƒê·∫¶U ---
                setTranscript(""); 
                setApiResult(null); // Reset k·∫øt qu·∫£ c≈© khi b·∫Øt ƒë·∫ßu n√≥i m·ªõi
                transcriptRef.current = "";
                recognitionRef.current.start();
                console.log("üî¥ B·∫ÆT ƒê·∫¶U thu √¢m...");
            } else {
                // --- K·∫æT TH√öC ---
                recognitionRef.current.stop();
                console.log("üõë ƒê√É T·∫ÆT MIC.");
                
                const finalContent = transcriptRef.current;
                console.log("Final content ", finalContent);

                // === G·ªåI API AI COMMAND ===
                if (finalContent && finalContent.trim()) {
                    console.log("ƒê√£ thu ƒë∆∞·ª£c v√† v√†o if")
                    try {
                        console.log("üöÄ ƒêang g·ª≠i text t·ªõi AI API:", finalContent);
                        
                        const aiRes = await fetch("http://localhost:3000/api/ai/command", {
                            method: "POST",
                            headers: {
                                "Content-Type": "application/json",
                            },
                            body: JSON.stringify({ text: finalContent }),
                        });

                        // 2. X·ª≠ l√Ω d·ªØ li·ªáu tr·∫£ v·ªÅ
                        if (aiRes.ok) {
                            const data = await aiRes.json();
                            console.log("‚úÖ API AI tr·∫£ v·ªÅ:", data); // Log ra console (F12)
                            setApiResult(data); // L∆∞u v√†o state ƒë·ªÉ hi·ªán l√™n m√†n h√¨nh
                        } else {
                            console.error("‚ùå API l·ªói:", aiRes.status);
                            setApiResult({ error: `L·ªói API: ${aiRes.status}` });
                        }
                    } catch (error) {
                        console.error("‚ùå L·ªói k·∫øt n·ªëi:", error);
                        setApiResult({ error: "Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn server" });
                    }
                }
            }
        }
        
        setIsRecording(!isRecording);

        // G·ªçi API Toggle microphone (gi·ªØ nguy√™n)
        try {
            await fetch(`http://localhost:3000/api/device/statusToggle/microphone`, {
                method: 'POST',
                credentials: "include"
            });
        } catch (error) {
            console.error('API Device Toggle Error:', error);
        }
    };

    return (
        <div className="w-xs h-fit bg-white rounded-xl p-3 flex flex-col gap-3">
            <h1 className="text-center font-semibold text-2xl">Microphone</h1>

            <div className="flex justify-center">
                <GrMicrophone
                    size={175}
                    className={
                        "p-5 cursor-pointer rounded-full transition " +
                        (isRecording ? "text-red-500 bg-red-100" : "hover:bg-gray-200")
                    }
                    onClick={handleToggleRecord}
                />
            </div>

            {/* Hi·ªÉn th·ªã Text ng∆∞·ªùi d√πng n√≥i */}
            {transcript && (
                <div className="p-3 bg-gray-100 rounded-lg text-sm border border-gray-300">
                    <b className="text-gray-600">B·∫°n ƒë√£ n√≥i:</b>
                    <p className="mt-1 text-gray-800">{transcript}</p>
                </div>
            )}

            {/* 3. Hi·ªÉn th·ªã K·∫øt qu·∫£ t·ª´ API AI (JSON) */}
            {apiResult && (
                <div className="p-3 bg-blue-50 rounded-lg text-sm border border-blue-200 overflow-x-auto">
                    <b className="text-blue-700">Ph·∫£n h·ªìi t·ª´ AI:</b>
                    <pre className="mt-2 text-xs text-blue-900 font-mono whitespace-pre-wrap">
                        {JSON.stringify(apiResult, null, 2)}
                    </pre>
                </div>
            )}
        </div>
    );
}