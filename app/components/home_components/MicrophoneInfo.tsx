// import { useEffect, useState } from "react";
// import { GrMicrophone } from "react-icons/gr";

// export default function MicrophoneInfo() {
//     const [powerMode, setPowerMode] = useState(false);
//     useEffect(() => {
//         const getMicInfo = async () => {
//             try {
//                 const res = await fetch(`http://localhost:3000/api/device/getInfoDevice/microphone`, {
//                     method: 'GET',
//                     credentials: "include"
//                 });
//                 if (!res.ok)
//                     throw new Error(`${res.status}`);
//                 const data = await res.json();
//                 setPowerMode(data.isActive);
//             } catch (error) {
//                 console.error(error);
//             }    
//         }
//         getMicInfo();
//     }, [])
//     return (<div className="w-xs h-fit bg-white rounded-xl p-3 ">
//         <h1 className="text-center font-semibold text-2xl">Microphone</h1>
//         <div className="flex justify-center">
//             <GrMicrophone size={175} className={"p-5 cursor-pointer hover:bg-gray-200 rounded-full m-5 " + (powerMode && "text-blue-400")}
//                 onClick={async () => {
//                     setPowerMode(!powerMode);
//                     try {
//                         const data = await fetch(`http://localhost:3000/api/device/statusToggle/microphone`, {
//                             method: 'POST',
//                             credentials: "include"
//                         });
//                     } catch (error) {
//                         console.error('Error: ', error);
//                     }
//                 }}
//             />
//         </div>
//     </div>)
// }

import { useEffect, useState, useRef } from "react";
import { GrMicrophone } from "react-icons/gr";

export default function MicrophoneInfo() {
    const [isRecording, setIsRecording] = useState(false);
    const [transcript, setTranscript] = useState("");

    // D√πng ref ƒë·ªÉ l∆∞u gi√° tr·ªã transcript m·ªõi nh·∫•t ph·ª•c v·ª• cho vi·ªác log khi t·∫Øt
    // (V√¨ state trong h√†m ƒë√≥ng event listener ƒë√¥i khi kh√¥ng c·∫≠p nh·∫≠t k·ªãp ƒë·ªÉ log)
    const transcriptRef = useRef(""); 
    const recognitionRef = useRef<any>(null);

    useEffect(() => {
        const SpeechRecognition =
            (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;

        if (!SpeechRecognition) {
            alert("Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ SpeechRecognition API");
            return;
        }

        const recognition = new SpeechRecognition();
        recognition.lang = "vi-VN";            
        recognition.continuous = true;         
        recognition.interimResults = false;    

        recognition.onresult = (event: any) => {
            let chunk = "";
            for (let i = event.resultIndex; i < event.results.length; i++) {
                if (event.results[i].isFinal) {
                    chunk += event.results[i][0].transcript + " ";
                }
            }

            if (chunk.trim() !== "") {
                // LOG 1: Log ngay ƒëo·∫°n v·ª´a n√≥i xong
                console.log("ü¶ª V·ª´a nghe ƒë∆∞·ª£c:", chunk);
                
                setTranscript((prev) => {
                    const newText = prev + chunk;
                    transcriptRef.current = newText; // C·∫≠p nh·∫≠t ref ƒë·ªÉ log sau
                    return newText;
                });
            }
        };

        recognition.onerror = (e: any) => console.error("Speech error:", e);
        recognitionRef.current = recognition;
    }, []);

    const handleToggleRecord = async () => {
        if (recognitionRef.current) {
            if (!isRecording) {
                // --- B·∫ÆT ƒê·∫¶U ---
                setTranscript(""); 
                transcriptRef.current = "";
                recognitionRef.current.start();
                console.log("üî¥ B·∫ÆT ƒê·∫¶U thu √¢m...");
            } else {
                // --- K·∫æT TH√öC ---
                recognitionRef.current.stop();
                
                // LOG 2: Log t·ªïng k·∫øt to√†n b·ªô n·ªôi dung
                console.log("üõë ƒê√É T·∫ÆT MIC. T·ªïng n·ªôi dung thu ƒë∆∞·ª£c:");
                console.log("üëâ " + (transcriptRef.current || "Ch∆∞a n√≥i g√¨ ho·∫∑c ch∆∞a nh·∫≠n di·ªán ƒë∆∞·ª£c"));
            }
        }
        
        const nextState = !isRecording;
        setIsRecording(nextState);

        // G·ªçi API Toggle
        try {
            // console.log("G·ªçi API toggle microphone...");
            const res = await fetch(`http://localhost:3000/api/device/statusToggle/microphone`, {
                method: 'POST',
                credentials: "include"
            });
            // if (res.ok) console.log("API Toggle OK");
        } catch (error) {
            console.error('API Error:', error);
        }
    };

    return (
        <div className="w-xs h-fit bg-white rounded-xl p-3">
            <h1 className="text-center font-semibold text-2xl">Microphone</h1>

            <div className="flex justify-center">
                <GrMicrophone
                    size={175}
                    className={
                        "p-5 cursor-pointer rounded-full m-5 transition " +
                        (isRecording ? "text-red-500 bg-red-100" : "hover:bg-gray-200")
                    }
                    onClick={handleToggleRecord}
                />
            </div>

            {/* Hi·ªÉn th·ªã text tr√™n giao di·ªán */}
            {transcript && (
                <div className="p-3 bg-gray-100 rounded-lg text-sm max-h-40 overflow-y-auto border border-gray-300">
                    <b className="text-gray-600">K·∫øt qu·∫£ (Real-time):</b>
                    <p className="mt-1 text-gray-800 font-medium">{transcript}</p>
                </div>
            )}
        </div>
    );
}